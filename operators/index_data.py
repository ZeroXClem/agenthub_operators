import tiktoken
import numpy as np
import openai
from itertools import islice

from .base_operator import BaseOperator
from .util import get_max_tokens_for_model

from ai_context import AiContext


# This is the size of each chunk in terms of tokens.I chose 1k because a small query could 
# hypothetically fit 4 chunks in the context. Feels like a good balance between speed and accuracy.
EMBEDDING_CTX_LENGTH = 1000
EMBEDDING_ENCODING = 'cl100k_base'


class IndexData(BaseOperator):
    @staticmethod
    def declare_name():
        return 'Index Data'
    
    @staticmethod
    def declare_category():
        return BaseOperator.OperatorCategory.MANIPULATE_DATA.value
    
    @staticmethod    
    def declare_parameters():
        return []
    
    @staticmethod    
    def declare_inputs():
        return [
            {
                "name": "text",
                "data_type": "string",
            }
        ]
    
    @staticmethod    
    def declare_outputs():
        return [
            {
                "name": "vector_index",
                # Just a dictionary, without specifying any keys that are expected to be present there.
                # Naturally since it is vector index, the keys are going to be str(embedding vector).
                "data_type": "{}",
            }
        ]

    def run_step(
        self,
        step,
        ai_context: AiContext
    ):
        text = ai_context.get_input('text', self)
        text = self.clean_text(text)
        embeddings_dict = self.len_safe_get_embedding(text, ai_context)
        ai_context.set_output('vector_index', embeddings_dict, self)
        ai_context.add_to_log("Indexing complete with {} chunk embeddings".format(len(embeddings_dict)))
    
    
    def clean_text(self, text):
        return text.replace("\n", " ")


    def batched(self, iterable, n):
        if n < 1:
            raise ValueError('n must be at least one')
        it = iter(iterable)
        while (batch := tuple(islice(it, n))):
            yield batch


    def chunked_tokens(self, text, encoding_name, chunk_length):
        encoding = tiktoken.get_encoding(encoding_name)
        tokens = encoding.encode(text)
        chunks_iterator = self.batched(tokens, chunk_length)
        for chunk in chunks_iterator:
            decoded_chunk = encoding.decode(chunk)  # Decode the chunk
            yield decoded_chunk


    def len_safe_get_embedding(
        self, 
        text, 
        ai_context,
        max_tokens=EMBEDDING_CTX_LENGTH, 
        encoding_name=EMBEDDING_ENCODING
    ):
        chunk_embeddings = {}
        for chunk in self.chunked_tokens(text, encoding_name=encoding_name, chunk_length=max_tokens):
            embedding = ai_context.embed_text(chunk)
            embedding_key = tuple(embedding)  # Convert numpy array to tuple
            chunk_embeddings[embedding_key] = chunk

        return chunk_embeddings

